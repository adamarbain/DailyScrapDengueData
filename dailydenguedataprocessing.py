# -*- coding: utf-8 -*-
"""DailyDengueDataProcessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QnpVOHB9yTaXAm0NDbE3wh1Q_zrmg7jP

Importing packages needed
"""

import requests
import pandas as pd
import datetime
import time
import numpy as np
import matplotlib.pyplot as plt
import geopandas as gpd
from shapely.geometry import Point, Polygon
import folium
import plotly.express as px
import os
import time
from datetime import datetime, timedelta
import pytz
import json
import csv

"""Weather API Functions for Open Meteo Integration"""

def fetch_weather_data(latitude, longitude, date, timezone="Asia/Singapore", max_retries=3):
    """
    Fetch weather data from Open Meteo Forecast API for a specific location and date
    
    Args:
        latitude (float): Latitude coordinate
        longitude (float): Longitude coordinate
        date (str): Date in format 'DD/MM/YYYY'
        timezone (str): Timezone for the API request
        max_retries (int): Maximum number of retry attempts
        
    Returns:
        dict: Weather data containing humidity, temperature, and rainfall
    """
    # Always use forecast API to avoid date range restrictions
    return fetch_weather_data_forecast(latitude, longitude, date, timezone, max_retries)

def fetch_weather_data_forecast(latitude, longitude, date, timezone="Asia/Singapore", max_retries=3):
    """
    Fetch weather data from Open Meteo Forecast API for any date (historical or future)
    
    Args:
        latitude (float): Latitude coordinate
        longitude (float): Longitude coordinate
        date (str): Date in format 'DD/MM/YYYY'
        timezone (str): Timezone for the API request
        max_retries (int): Maximum number of retry attempts
        
    Returns:
        dict: Weather data containing humidity, temperature, and rainfall
    """
    for attempt in range(max_retries):
        try:
            # Convert date from DD/MM/YYYY to YYYY-MM-DD format
            date_obj = datetime.strptime(date, '%d/%m/%Y')
            formatted_date = date_obj.strftime('%Y-%m-%d')
            
            # Check if date is today or in the future
            today = datetime.now().date()
            if date_obj.date() == today:
                print(f"Info: Date {date} is today. Using forecast API.")
            elif date_obj.date() > today:
                print(f"Info: Date {date} is in the future. Using forecast API.")
            else:
                print(f"Info: Date {date} is historical. Using forecast API.")
            
            # Construct Forecast API URL - always use forecast API
            base_url = "https://api.open-meteo.com/v1/forecast"
            params = {
                'latitude': latitude,
                'longitude': longitude,
                'daily': 'precipitation_sum,temperature_2m_max,temperature_2m_min',
                'hourly': 'relative_humidity_2m',
                'timezone': timezone,
                'start_date': formatted_date,
                'end_date': formatted_date
            }
            
            # Make API request with timeout
            response = requests.get(base_url, params=params, timeout=60)
            response.raise_for_status()
            
            data = response.json()
            
            # Extract weather data
            weather_data = {
                'humidity': None,
                'temperature': None,
                'rainfall': None
            }
            
            # Calculate average humidity from hourly data
            if 'hourly' in data and 'relative_humidity_2m' in data['hourly']:
                humidity_values = data['hourly']['relative_humidity_2m']
                if humidity_values and all(v is not None for v in humidity_values):
                    weather_data['humidity'] = sum(humidity_values) / len(humidity_values)
            
            # Extract daily temperature (average of max and min) and rainfall
            if 'daily' in data:
                if 'temperature_2m_max' in data['daily'] and 'temperature_2m_min' in data['daily']:
                    temp_max = data['daily']['temperature_2m_max'][0] if data['daily']['temperature_2m_max'] else None
                    temp_min = data['daily']['temperature_2m_min'][0] if data['daily']['temperature_2m_min'] else None
                    if temp_max is not None and temp_min is not None:
                        weather_data['temperature'] = (temp_max + temp_min) / 2
                
                if 'precipitation_sum' in data['daily'] and data['daily']['precipitation_sum']:
                    weather_data['rainfall'] = data['daily']['precipitation_sum'][0]
            
            return weather_data
            
        except requests.exceptions.Timeout as e:
            print(f"Forecast API timeout on attempt {attempt + 1}/{max_retries} for lat={latitude}, lon={longitude}, date={date}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                continue
            else:
                print(f"All forecast retry attempts failed for lat={latitude}, lon={longitude}, date={date}")
                return {'humidity': None, 'temperature': None, 'rainfall': None}
        except requests.exceptions.RequestException as e:
            print(f"Forecast API request failed for lat={latitude}, lon={longitude}, date={date}: {str(e)}")
            return {'humidity': None, 'temperature': None, 'rainfall': None}
        except Exception as e:
            print(f"Error processing forecast weather data for lat={latitude}, lon={longitude}, date={date}: {str(e)}")
            return {'humidity': None, 'temperature': None, 'rainfall': None}

def fetch_weather_data_archive(latitude, longitude, date, timezone="Asia/Singapore", max_retries=3):
    """
    Fetch historical weather from Open-Meteo Archive API for a specific date and location.
    Uses hourly humidity to compute daily average humidity, and daily precipitation_sum.
    For temperature, prefers daily temperature_2m_mean, else average of max/min.
    """
    for attempt in range(max_retries):
        try:
            date_obj = datetime.strptime(date, '%d/%m/%Y')
            formatted_date = date_obj.strftime('%Y-%m-%d')

            base_url = "https://archive-api.open-meteo.com/v1/archive"
            params = {
                'latitude': latitude,
                'longitude': longitude,
                'start_date': formatted_date,
                'end_date': formatted_date,
                'timezone': timezone,
                'daily': 'precipitation_sum,temperature_2m_mean,temperature_2m_max,temperature_2m_min',
                'hourly': 'relative_humidity_2m'
            }

            response = requests.get(base_url, params=params, timeout=60)
            response.raise_for_status()
            data = response.json()

            weather_data = {
                'humidity': None,
                'temperature': None,
                'rainfall': None
            }

            # Humidity from hourly average
            if 'hourly' in data and 'relative_humidity_2m' in data['hourly']:
                humidity_values = data['hourly']['relative_humidity_2m']
                if humidity_values and all(v is not None for v in humidity_values):
                    weather_data['humidity'] = sum(humidity_values) / len(humidity_values)

            # Temperature from daily mean if available, else avg of max/min
            if 'daily' in data:
                daily = data['daily']
                temp_mean = daily.get('temperature_2m_mean')
                if temp_mean and len(temp_mean) > 0 and temp_mean[0] is not None:
                    weather_data['temperature'] = temp_mean[0]
                else:
                    tmax = daily.get('temperature_2m_max')
                    tmin = daily.get('temperature_2m_min')
                    if tmax and tmin and len(tmax) > 0 and len(tmin) > 0 and tmax[0] is not None and tmin[0] is not None:
                        weather_data['temperature'] = (tmax[0] + tmin[0]) / 2

                precip = daily.get('precipitation_sum')
                if precip and len(precip) > 0:
                    weather_data['rainfall'] = precip[0]

            return weather_data

        except requests.exceptions.Timeout:
            print(f"Archive API timeout on attempt {attempt + 1}/{max_retries} for lat={latitude}, lon={longitude}, date={date}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            return {'humidity': None, 'temperature': None, 'rainfall': None}
        except requests.exceptions.RequestException as e:
            print(f"Archive API request failed for lat={latitude}, lon={longitude}, date={date}: {str(e)}")
            return {'humidity': None, 'temperature': None, 'rainfall': None}
        except Exception as e:
            print(f"Error processing archive weather data for lat={latitude}, lon={longitude}, date={date}: {str(e)}")
            return {'humidity': None, 'temperature': None, 'rainfall': None}

"""Updating Files in GitHub Repository


"""

# ðŸ“Œ Step 1: Define paths relative to the GitHub repository
dengue_hotspot_csv = "dengue_hotspot.csv"
active_dengue_csv = "active_dengue.csv"

# ðŸ“Œ Step 2: Helper function to save DataFrame to CSV
def save_to_csv(df, file_path):
    if df is None or len(df) == 0:
        print("No data to save.")
        return

    if os.path.exists(file_path):
        # Read existing header to detect schema
        try:
            existing_columns = list(pd.read_csv(file_path, nrows=0).columns)
        except Exception as e:
            existing_columns = []

        new_columns = list(df.columns)

        if existing_columns:
            # If schema changed, rewrite file with unified columns
            if existing_columns != new_columns:
                combined_columns = existing_columns + [c for c in new_columns if c not in existing_columns]

                try:
                    existing_df = pd.read_csv(file_path)
                except Exception:
                    existing_df = pd.DataFrame(columns=existing_columns)

                existing_df = existing_df.reindex(columns=combined_columns)
                df_to_save = df.reindex(columns=combined_columns)
                combined_df = pd.concat([existing_df, df_to_save], ignore_index=True)
                combined_df.to_csv(file_path, index=False)
                print(f"Schema updated and data merged into {file_path}")
            else:
                df.to_csv(file_path, mode="a", header=False, index=False)
                print(f"Data appended to {file_path}")
        else:
            df.to_csv(file_path, index=False)
            print(f"Data saved to {file_path}")
    else:
        # New file - write with header
        df.to_csv(file_path, index=False)
        print(f"Data saved to {file_path}")

"""API 1 : Fetch UM Location from Idengue.com"""

um_location_url = "https://geocode.arcgis.com/arcgis/rest/services/World/GeocodeServer/findAddressCandidates?SingleLine=Universiti%20Malaya%2C%20Kuala%20Lumpur%2C%20Wilayah%20Persekutuan%20Kuala%20Lumpur%2C%20MYS&f=json&outSR=%7B%22wkid%22%3A102100%7D&outFields=*&magicKey=dHA9MCN0dj00Yjg3MGE5MSNsb2M9Njc4NTA0MTEjbG5nPTExMiNwbD04NTk4Mjk0MCNsYnM9MTQ6NzM1MzMxODMjbG49V29ybGQ%3D&maxLocations=6"

"""API 2 : List of Hotspot"""

hotspot_location_url = "https://sppk.mysa.gov.my/proxy/proxy.php?https://mygis.mysa.gov.my/erica1/rest/services/iDengue/WM_idengue/MapServer/0/query?f=json&where=1%3D1&returnGeometry=true&spatialRel=esriSpatialRelIntersects&outFields=SPWD.AVT_HOTSPOTMINGGUAN.KUMULATIF_KES%2CSPWD.AVT_HOTSPOTMINGGUAN.TEMPOH_WABAK%2CSPWD.AVT_HOTSPOTMINGGUAN.NEGERI%2CSPWD.AVT_HOTSPOTMINGGUAN.DAERAH%2CSPWD.DBO_LOKALITI_POINTS.LOKALITI"

"""API 3 : List of Kawasan Wabak Aktif"""

active_area_url = "https://sppk.mysa.gov.my/proxy/proxy.php?https://mygis.mysa.gov.my/erica1/rest/services/iDengue/WM_idengue/MapServer/4/query?f=json&where=1%3D1&returnGeometry=true&spatialRel=esriSpatialRelIntersects&outFields=SPWD.AVT_WABAK_IDENGUE_NODM.LOKALITI%2CSPWD.AVT_WABAK_IDENGUE_NODM.TOTAL_KES%2CSPWD.AVT_WABAK_IDENGUE_NODM.NEGERI"

"""Defining API Endpoint"""

# Define API endpoints
API_ENDPOINTS = {
    "um_location": um_location_url,
    "hotspot_location_url" : hotspot_location_url,
    "active_area_url" : active_area_url,
}
print(API_ENDPOINTS)

"""Set Timezone to Malaysia (UTC+8)"""

# Set timezone to Malaysia (UTC+8)
malaysia_tz = pytz.timezone("Asia/Kuala_Lumpur")

"""Define functions for processing different API responses"""

def process_api_1(response_json):
    """Process and visualize data for API 1 with interactive hover tooltips."""
    candidates = response_json.get("candidates", [])

    data = [
        {
            "X": c["attributes"].get("X"),
            "Y": c["attributes"].get("Y"),
            "attributes": c["attributes"],
            "location": c["attributes"].get("LongLabel"),
        }
        for c in candidates
    ]

    if not data:
        print("No data available for API 1.")
        return None

    df = pd.DataFrame(data)

    print(f"API 1: Found {len(df)} locations.")
    for idx, row in df.iterrows():
        print(f"Location {idx + 1}: Location -> {row['location']}")

    # Convert attributes dict to string for display
    df["attributes_str"] = df["attributes"].astype(str)

    # Create interactive scatter plot with hover tooltip
    fig = px.scatter(
        df,
        x="X",
        y="Y",
        labels={"X": "Longitude", "Y": "Latitude"},
        title="API 1 - UM Location",
        color_discrete_sequence=["blue"]
    )

    fig.show()

    return df

def process_api_2(response_json, x_target=101.653045, y_target=3.122496, tolerance=0.045):
    """Process and visualize data for API 2 with interactive hover tooltips and weather data."""
    features = response_json.get("features", [])

    filtered_data = []
    current_date = datetime.now(malaysia_tz).strftime('%d/%m/%Y')  # Malaysia Time

    for feature in features:
        attr = feature["attributes"]
        area = attr.get("SPWD.DBO_LOKALITI_POINTS.LOKALITI", "Unknown")
        state = attr.get("SPWD.AVT_HOTSPOTMINGGUAN.NEGERI", "Unknown")
        days_duration = attr.get("SPWD.AVT_HOTSPOTMINGGUAN.TEMPOH_WABAK", 0)
        total_cases = attr.get("SPWD.AVT_HOTSPOTMINGGUAN.KUMULATIF_KES", 0)

        if (x_target - tolerance <= feature["geometry"]["x"] <= x_target + tolerance) and \
           (y_target - tolerance <= feature["geometry"]["y"] <= y_target + tolerance):
            
            # Fetch weather data for this location and date
            print(f"Fetching weather data for hotspot: {area}")
            weather_data = fetch_weather_data(feature["geometry"]["y"], feature["geometry"]["x"], current_date)
            
            # Ensure weather data has valid values
            if weather_data is None:
                weather_data = {'humidity': None, 'temperature': None, 'rainfall': None}
            
            filtered_data.append({
                "x": feature["geometry"]["x"],
                "y": feature["geometry"]["y"],
                "date": current_date,
                "area": area,
                "state": state,
                "days_duration": days_duration,
                "total_active_cases": total_cases,
                "humidity": weather_data.get('humidity'),
                "temperature": weather_data.get('temperature'),
                "rainfall": weather_data.get('rainfall')
            })
            
            # Add delay to avoid rate limiting
            time.sleep(1)

    if not filtered_data:
        print("No matching data found within the specified range.")
        return None

    df = pd.DataFrame(filtered_data)

    print(f"API 2: Found {len(df)} dengue hotspot locations.")
    for idx, row in df.iterrows():
        print(f"Hotspot {idx + 1}: Area -> {row['area']}, Cases -> {row['total_active_cases']}")
        print(f"  Weather: Humidity={row['humidity']:.2f}%, Temp={row['temperature']:.2f}Â°C, Rain={row['rainfall']:.2f}mm")

    # Create interactive scatter plot with hover tooltip
    fig = px.scatter(
        df,
        x="x",
        y="y",
        hover_data=["area", "total_active_cases", "humidity", "temperature", "rainfall"],
        labels={"x": "Longitude", "y": "Latitude"},
        title="API 2 - Dengue Hotspots (5KM Radius) with Weather Data",
        color_discrete_sequence=["red"]
    )

    # Add target location as a marker
    fig.add_scatter(
        x=[x_target],
        y=[y_target],
        mode="markers",
        marker=dict(size=10, color="black", symbol="x"),
        name="Target Location"
    )

    fig.show()

    # Save to Google Drive
    save_to_csv(df, dengue_hotspot_csv)

    return df

def calculate_centroid(rings):
    """Calculate the centroid of a polygon given its rings."""
    all_x = [point[0] for ring in rings for point in ring]
    all_y = [point[1] for ring in rings for point in ring]
    return np.mean(all_x), np.mean(all_y)

def process_api_3(response_json, x_target=101.653045, y_target=3.122496, tolerance=0.045):
    """Process and visualize data for API 3, filtering based on polygon centroid with weather data."""
    features = response_json.get("features", [])

    filtered_data = []
    current_date = datetime.now(malaysia_tz).strftime('%d/%m/%Y')  # Malaysia Time
    
    for feature in features:
        rings = feature.get("geometry", {}).get("rings", [])
        if not rings:
            continue

        centroid_x, centroid_y = calculate_centroid(rings)

        # Extract relevant attributes
        attributes = feature.get("attributes", {})
        location = attributes.get("SPWD.AVT_WABAK_IDENGUE_NODM.LOKALITI", "null")
        state = attributes.get("SPWD.AVT_WABAK_IDENGUE_NODM.NEGERI", "null")
        total_cases = attributes.get("SPWD.AVT_WABAK_IDENGUE_NODM.TOTAL_KES", 0)

        if (x_target - tolerance <= centroid_x <= x_target + tolerance) and (y_target - tolerance <= centroid_y <= y_target + tolerance):
            
            # Fetch weather data for this location and date
            print(f"Fetching weather data for active area: {location}")
            weather_data = fetch_weather_data(centroid_y, centroid_x, current_date)
            
            # Ensure weather data has valid values
            if weather_data is None:
                weather_data = {'humidity': None, 'temperature': None, 'rainfall': None}
            
            filtered_data.append({
                "attributes": feature["attributes"],
                "centroid_x": centroid_x,
                "centroid_y": centroid_y,
                "date": current_date,
                "location": location,
                "state": state,
                "total_active_cases": total_cases,
                "humidity": weather_data.get('humidity'),
                "temperature": weather_data.get('temperature'),
                "rainfall": weather_data.get('rainfall')
            })
            
            # Add delay to avoid rate limiting
            time.sleep(1)

    if not filtered_data:
        print("No matching data found within the specified range.")
        return None

    df = pd.DataFrame(filtered_data)

    print(f"API 3: Found {len(df)} active area centroids.")
    for idx, row in df.iterrows():
        print(f"Centroid {idx + 1}: Location: {row['location']}, Total Cases: {row['total_active_cases']}")
        print(f"  Weather: Humidity={row['humidity']:.2f}%, Temp={row['temperature']:.2f}Â°C, Rain={row['rainfall']:.2f}mm")

    # Convert attributes dict to string for display
    df["attributes_str"] = df["attributes"].astype(str)

    # Create interactive scatter plot with hover tooltip
    fig = px.scatter(
        df,
        x="centroid_x",
        y="centroid_y",
        hover_data=["location", "state", "total_active_cases", "humidity", "temperature", "rainfall"],
        labels={"centroid_x": "Longitude", "centroid_y": "Latitude"},
        title="API 3 - Active Area Centroids (5KM Radius) with Weather Data"
    )

    # Add target location as a marker
    fig.add_scatter(
        x=[x_target],
        y=[y_target],
        mode="markers",
        marker=dict(size=10, color="black", symbol="x"),
        name="Target Location"
    )

    fig.show()

    # Drop the unwanted columns
    df = df.drop(columns=["attributes", "attributes_str"])

    # Save to Google Drive
    save_to_csv(df, active_dengue_csv)

    return df

"""Mapping APIs to their respective processing functions"""

PROCESSING_FUNCTIONS = {
    "um_location": process_api_1,
    "hotspot_location_url": process_api_2,
    "active_area_url": process_api_3
}
print(PROCESSING_FUNCTIONS)

"""Define function to fetch and store the data"""

def fetch_and_store(api_name, url):
    """Fetch data from API and store it using the appropriate processing function."""
    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()

        if api_name in PROCESSING_FUNCTIONS:
            df = PROCESSING_FUNCTIONS[api_name](data)
            print(f"Data from {api_name} processed successfully!")
            return df
        else:
            print(f"No processing function defined for {api_name}")
    except Exception as e:
        print(f"Error fetching data from {api_name}: {e}")

"""Weather Data Summary Function"""

def display_weather_summary(df, api_name):
    """Display weather data summary for the processed data."""
    if df is None or len(df) == 0:
        print(f"No weather data available for {api_name}")
        return
    
    print(f"\n=== Weather Data Summary for {api_name} ===")
    
    # Check if weather columns exist
    weather_columns = ['humidity', 'temperature', 'rainfall']
    available_weather_cols = [col for col in weather_columns if col in df.columns]
    
    if not available_weather_cols:
        print("No weather data columns found in the dataset.")
        return
    
    for col in available_weather_cols:
        if df[col].notna().any():
            print(f"{col.capitalize()}:")
            print(f"  - Records with data: {df[col].notna().sum()}/{len(df)}")
            print(f"  - Average: {df[col].mean():.2f}")
            print(f"  - Min: {df[col].min():.2f}")
            print(f"  - Max: {df[col].max():.2f}")
        else:
            print(f"{col.capitalize()}: No data available")
    
    print("=" * 50)

"""Backfill historical weather for existing CSVs"""

def backfill_weather_for_csv(file_path, lon_col="x", lat_col="y", date_col="date", sleep_seconds=0.2):
    """
    Read an existing CSV (e.g., dengue_hotspot.csv), fetch historical weather
    for each unique (lat, lon, date), and write back with humidity, temperature, rainfall.
    """
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return

    # Robust read using csv module to handle mixed-width rows
    try:
        with open(file_path, "r", newline="", encoding="utf-8") as f:
            reader = csv.reader(f)
            rows = list(reader)
    except Exception as e:
        print(f"Failed to read {file_path}: {e}")
        return

    if not rows:
        print(f"No rows found in {file_path}")
        return

    header = rows[0]
    data_rows = rows[1:]

    # Determine the maximum number of columns present across all rows
    max_len = max(len(r) for r in ([header] + data_rows))

    # If there are extra columns without header names, extend header
    if max_len > len(header):
        # Heuristically name extra columns as humidity, temperature, rainfall (in that order)
        needed = max_len - len(header)
        extras = []
        candidates = ["humidity", "temperature", "rainfall"]
        for i in range(needed):
            extras.append(candidates[i] if i < len(candidates) else f"extra_{i}")
        header = header + extras

    # Normalize all data rows to the header length
    normalized = []
    for r in data_rows:
        if len(r) < len(header):
            r = r + [""] * (len(header) - len(r))
        elif len(r) > len(header):
            r = r[:len(header)]
        normalized.append(r)

    # Build DataFrame
    df = pd.DataFrame(normalized, columns=header)

    # Validate required columns
    for required in [lon_col, lat_col, date_col]:
        if required not in df.columns:
            print(f"Missing required column '{required}' in {file_path}")
            return

    # Ensure weather columns exist
    for col in ["humidity", "temperature", "rainfall"]:
        if col not in df.columns:
            df[col] = np.nan
        else:
            # Normalize empty strings to NaN so we don't mistake them as filled
            df[col] = df[col].replace("", np.nan)

    # Cache to avoid repeated API calls for the same (lat, lon, date)
    weather_cache = {}

    def cache_key(lat_val, lon_val, date_str):
        try:
            return (round(float(lat_val), 6), round(float(lon_val), 6), str(date_str))
        except Exception:
            return (str(lat_val), str(lon_val), str(date_str))

    total_rows = len(df)
    print(f"Starting weather backfill for {total_rows} rows in {file_path}...")

    def is_filled(val):
        return pd.notna(val) and str(val).strip() != ""

    for idx, row in df.iterrows():
        # Skip rows that already have non-empty weather values
        if is_filled(row.get("humidity")) and is_filled(row.get("temperature")) and is_filled(row.get("rainfall")):
            continue

        lat_val = row[lat_col]
        lon_val = row[lon_col]
        date_str = row[date_col]

        key = cache_key(lat_val, lon_val, date_str)

        if key in weather_cache:
            weather = weather_cache[key]
        else:
            # Use archive API for past dates backfill
            weather = fetch_weather_data_archive(lat_val, lon_val, date_str)
            weather_cache[key] = weather
            time.sleep(sleep_seconds)

        if isinstance(weather, dict):
            df.at[idx, "humidity"] = weather.get("humidity")
            df.at[idx, "temperature"] = weather.get("temperature")
            df.at[idx, "rainfall"] = weather.get("rainfall")

        if (idx + 1) % 200 == 0:
            print(f"Processed {idx + 1}/{total_rows} rows...")

    # Reorder columns to keep original first, then weather columns at the end (once each)
    base_cols = [c for c in df.columns if c not in ["humidity", "temperature", "rainfall"]]
    ordered_cols = base_cols + ["humidity", "temperature", "rainfall"]
    df = df[ordered_cols]

    # Write atomically
    tmp_path = f"{file_path}.tmp"
    try:
        df.to_csv(tmp_path, index=False)
        os.replace(tmp_path, file_path)
        print(f"Backfill completed and saved to {file_path}")
    finally:
        if os.path.exists(tmp_path):
            try:
                os.remove(tmp_path)
            except Exception:
                pass

"""Run daily data fetching with weather integration"""

if __name__ == "__main__":
    print("=" * 60)
    print("DAILY DENGUE DATA PROCESSING WITH WEATHER INTEGRATION")
    print("=" * 60)
    print("Fetching dengue data and weather information...")
    print("Note: This process may take time due to weather API calls.")
    print("=" * 60)
    
    results = {}
    
    for api_name, url in API_ENDPOINTS.items():
        print(f"\nFetching data from {api_name}...")
        df = fetch_and_store(api_name, url)
        results[api_name] = df
        
        # Display weather summary if applicable
        if api_name in ["hotspot_location_url", "active_area_url"] and df is not None:
            display_weather_summary(df, api_name)
    
    print("\n" + "=" * 60)
    print("DAILY DATA PROCESSING COMPLETED")
    print("=" * 60)
    
    # Final summary
    total_hotspots = len(results.get("hotspot_location_url", pd.DataFrame())) if results.get("hotspot_location_url") is not None else 0
    total_active_areas = len(results.get("active_area_url", pd.DataFrame())) if results.get("active_area_url") is not None else 0
    
    print(f"Total dengue hotspots found: {total_hotspots}")
    print(f"Total active outbreak areas found: {total_active_areas}")
    print(f"Data saved to: {dengue_hotspot_csv} and {active_dengue_csv}")
    print("All data now includes weather information (humidity, temperature, rainfall)")
